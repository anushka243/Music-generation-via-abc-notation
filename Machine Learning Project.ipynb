{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating ABC notation files with LSTM-RNN Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past few years, AI has helped significantly in revolutionizing music generation. The long term impact of this technological innovation has made it progressively easier for artists to realize their creative visions, resulting in AI being seen as a powerful tool and partner for the artists. Despite previous study in music generation through machine learning, there is still room to delve into and build sophisticated models. In this work, we use LSTM(RNN) over an “abc” notation to achieve efficient music production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from below is the code for training and LSTM(RNN) over our dataset. The dataset we have used is the Nottingham Music Database which has over 1000 folk tunes in ABC notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are treating the ABC notation files as general text files, we would be pre processing the data as any other text file.\n",
    "Along with pre processing the data, the following code also prepares batches for training.\n",
    "For pre processing the data we are mapping the characters with integer encoding by assigning a unique number based on the frequency of each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self,encoding='utf-8'):\n",
    "        # self.data_dir = data_dir\n",
    "        # self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(\"data\", \"input.txt\")\n",
    "        vocab_file = os.path.join(\"data\", \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(\"data\", \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    # preprocess data for the first time.\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "\n",
    "    # load the preprocessed the data if the data has been processed before.\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / 2500)\n",
    "    # seperate the whole data into different batches.\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / 2500)\n",
    "    \n",
    "\n",
    "        # When the data (tensor) is too small,\n",
    "        # let's give them a better error message\n",
    "        # if self.num_batches == 0:\n",
    "        #     assert False, \"Not enough data\"\n",
    "\n",
    "        # reshape the original data into the length self.num_batches * self.batch_size * self.seq_length for convenience.\n",
    "        self.tensor = self.tensor[:self.num_batches * 50 * 50]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "\n",
    "        #ydata is the xdata with one position shift.\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(50, -1),\n",
    "                                  self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(50, -1),\n",
    "                                  self.num_batches, 1)\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is where we define our training model. As told before, we would be using a Recurrent Neural Network with LSTM ( Long Short Term Memory ) cells.\n",
    "The core of the model consists of an LSTM cell that processes one word at a time and computes probabilities of the possible values for the next word in the sequence\n",
    "The characters are embedded into a dense vector representation before we feed it into the LSTM\n",
    "\n",
    "We are using an RNN of size 128 cells in each layer\n",
    "\n",
    "We have used softmax for activation in the output layer\n",
    "\n",
    "Loss Function:\n",
    "Our aim is to minimize the average negative log probability of the target words\n",
    "\n",
    "Optimizer:\n",
    "We have used Adam optimizer which keeps separate learning rates for each weight as well as an exponentially decaying average of previous gradients. This is the best optimizer for noisy data and hence we have used this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, vocab_size, training=True):\n",
    "        cell_fn = rnn.LSTMCell\n",
    "        cells = []\n",
    "        for _ in range(2):\n",
    "            cell = cell_fn(128)\n",
    "            cell = rnn.DropoutWrapper(cell,\n",
    "                                      input_keep_prob=1.0,\n",
    "                                      output_keep_prob=1.0)\n",
    "            cells.append(cell)\n",
    "        self.cell = cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # input/target data (int32 since input is char-level)\n",
    "        self.input_data = tf.placeholder(\n",
    "            tf.int32, [50,50])\n",
    "        self.targets = tf.placeholder(\n",
    "            tf.int32, [50,50])\n",
    "        self.initial_state = cell.zero_state(50, tf.float32)\n",
    "\n",
    "        # softmax output layer, use softmax to classify\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\",\n",
    "                                        [128, vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "\n",
    "        # transform input to embedding\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, 128])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        # dropout beta testing: double check which one should affect next line\n",
    "        # if training and args.output_keep_prob:\n",
    "        inputs = tf.nn.dropout(inputs, 1.0)\n",
    "\n",
    "        # unstack the input to fits in rnn model\n",
    "        inputs = tf.split(inputs, 50, 1)\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        # loop function for rnn_decoder, which take the previous i-th cell's output and generate the (i+1)-th cell's input\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        # rnn_decoder to generate the ouputs and final state. When we are not training the model, we use the loop function.\n",
    "        outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if not training else None, scope='rnnlm')\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, 128])\n",
    "\n",
    "        # output layer\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "        # loss is calculate by the log loss and taking the average.\n",
    "        loss = legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([2500])])\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.reduce_sum(loss) / 50 / 50\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        # calculate gradients\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                5.)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        # apply gradient change to the all the trainable variable.\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is where we train our dataset using the model above. \n",
    "\n",
    "We are using tensorboard to illustrate the variation of loss for each epoch. \n",
    "\n",
    "For the sake of easy training, we have kept the epoch=50. A higher value would have led to better predictions though we have been able to produce valid ABC notations with with epoch=50 and a relatively small dataset.\n",
    "\n",
    "We are saving our trained model at certain iterations just in case we wanted to halt the training process at some point. Prediction is done using the latest saved training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    data_loader = TextLoader()\n",
    "    vocab_size = data_loader.vocab_size\n",
    "    print(data_loader.num_batches)\n",
    "\n",
    "    if not os.path.isdir(\"save\"):\n",
    "        os.makedirs(\"save\")\n",
    "    with open(os.path.join(\"save\", 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "\n",
    "    model = Model(vocab_size=vocab_size)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # instrument for tensorboard\n",
    "        summaries = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\n",
    "                os.path.join(\"logs\", time.strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "       \n",
    "        for e in range(50):\n",
    "            sess.run(tf.assign(model.lr,\n",
    "                               0.002 * (0.97 ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state)\n",
    "            for b in range(data_loader.num_batches):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                print(feed)\n",
    "                for i, (c, h) in enumerate(model.initial_state):\n",
    "                    feed[c] = state[i].c\n",
    "                    feed[h] = state[i].h\n",
    "\n",
    "                # instrument for tensorboard\n",
    "                summ, train_loss, state, _ = sess.run([summaries, model.cost, model.final_state, model.train_op], feed)\n",
    "                writer.add_summary(summ, e * data_loader.num_batches + b)\n",
    "\n",
    "                end = time.time()\n",
    "                print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                      .format(e * data_loader.num_batches + b,\n",
    "                              50 * data_loader.num_batches,\n",
    "                              e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % 1000 == 0\\\n",
    "                        or (e == 49 and\n",
    "                            b == data_loader.num_batches-1):\n",
    "                    # save for the last result\n",
    "                    checkpoint_path = os.path.join(\"save\", 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path,\n",
    "                               global_step=e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Music Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have finished training our model all we need to do is to generate new characters based on our exisiting model.\n",
    "We see that after training our model for epochs=50 on the Nottingham Music Dataset, our RNN is able to produce valid abc notation files which can be converted to music(not everytime though)\n",
    "\n",
    "The following code takes the most recently saved trained model to predict a new sequence of characters (which is basically the abc notation)\n",
    "\n",
    "We have a pre trained model already in place in the \"save\" folder and hence new music can be produced by using those models by running the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    with open(os.path.join(\"save\", 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(\"save\", 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    model = Model(saved_args, training=False)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(\"save\")\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            f=open(\"new.txt\",\"a+\")\n",
    "            result=(model.sample(sess, chars, vocab, 1000, \"X:\",  #1000- number of characters produced, \"X:\"- prime character\n",
    "                               1).encode('utf-8')).decode('utf-8')\n",
    "            f.write(result)\n",
    "    f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some of the abc notation produced when generate.py is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X: 13\n",
    "\n",
    "T:Radmy Ball\n",
    "\n",
    "% Nottingham Music Database\n",
    "\n",
    "P:AAB\n",
    "\n",
    "S:Chris Dewhurst 19\n",
    "\n",
    "M:4/4\n",
    "\n",
    "L:1/8\n",
    "\n",
    "R:Hornpipe\n",
    "\n",
    "K:D\n",
    "\n",
    "P:A\n",
    "\n",
    "cf|\"D\"f2ed f2f2|\"Em\"g2f2 B2^d2|\"Em\"efgg \"D\"fede|\"G\"dcBA BdcB|\"gbe/2^f/2|\"Em\"edc|\"Em\"e2f|\n",
    "\n",
    "\"Am\"e3/2d/2c/2B/2|\"Am\"A3/2B/2A|\"D7\"A3/2F/2A|\"G\"B3/2c/2d|\"Em\"g2e|\"F\"f2a|\"Bm\"f3|\"C/e\"ed2 \"Am\"f2e|\"D7\"d2c|\n",
    "\n",
    "\"G\"dBG|\"Am\"F2A \"E7\"FED|\"D7\"A2G \"Am\"G2G|\"G\"F2G \"D7/a\"AFG|\n",
    "\n",
    "\"C\"c2e \"F\" a3|\"Bb\"dcB \"Dm\"A2F|\"G\"G2D G2B|\"Am\"e3 \"G7\"cBA|\"F#m\"B4 ||\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X: 1\n",
    "\n",
    "T:Dirne's Reel\n",
    "\n",
    "% Nottingham Music Database\n",
    "\n",
    "S:Trad\n",
    "\n",
    "M:3/4\n",
    "\n",
    "L:1/4\n",
    "\n",
    "K:Am\n",
    "\n",
    "P:A\n",
    "\n",
    "g|\"Am\"f/2e/2d/2c/2d/2c/2|\"G\"BB/2G/2F/2B/2|\"Em\"B/2A/2G/2E/2F/2G/2|\"Am\"A/2B/2A/2F/2G/2E/2:|\n",
    "\n",
    "\"D\"D/2E/2D/2D/2D/2E/2 d/2c/2A/2B/2A/2F/2||/2D/2D/2F/2A/2D/2 GD/2D/2|\\\n",
    "\n",
    "\"G\"E/2D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X: 12\n",
    "\n",
    "T:The Welm Brband P1y\n",
    "\n",
    "% Nottingham Music Database\n",
    "\n",
    "S:Chris Dewhurst 1955, via Phil Rowe\n",
    "\n",
    "M:6/8\n",
    "\n",
    "K:Gm\n",
    "\n",
    "D|\"G\"BGB GGB|\"C\"c2A G3|\"C\"(3cBcA G2A|\"Cm\"dcB ecG|\"D7\"F2D cFD|\n",
    "\n",
    "\"G\"DGB dBG|\"G\"GFG \"E7\"BAG|\"Am\"Ace \"Dm\"fed|\"Em\"edB \"G\"G\\\n",
    "\n",
    ":|\n",
    "\n",
    "K:Am\n",
    "\n",
    "P:/8C/8C/8Ep8g/8f/8\"A7\"d/2(3e d/2+ec cAA|\"Em\"e3 g3:|\n",
    "\n",
    "P:B\n",
    "\n",
    "(3G/2G/2B/2|\"A\"A2A \"C\"[c3g3|\"D\"F2A B3-|2c2 B2A|A2B A2B|\"A/c\"c2A \"E7\"^G2A|\n",
    "\n",
    "\"Am\"cBA c3|\"G\"dBG \"3\"G3:|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X: 67\n",
    "\n",
    "T:The The Fluenssamle Sireby\n",
    "\n",
    "% Nottingham Music Database\n",
    "\n",
    "S:Mick Peat\n",
    "\n",
    "M:4/4\n",
    "\n",
    "L:1/4\n",
    "\n",
    "K:D\n",
    "\n",
    "P:A\n",
    "\n",
    ":|:A|\"D\"df dg|\"A\"\"f#\"fd dB|\"Em\"d2 fe|\"D\"d2 -d2|\\\n",
    "\n",
    "\"A7\"c/2B/2=c fe|\"F#7\"Ag2e|\"D\"f2 f3/2f/2|\n",
    "\n",
    "\"Em\"ee \"E7\"ge|\"D\"a2 fA|\"D\"d2 d2|\"C\"eg gc|A3g|\"G\"d4|\"Em\"g2 -e/2d/2c|\n",
    "\n",
    "\"G\"BG \"D\"FE|\"D\"F2 FG|\"D\"F3/2G/2 Ad|\"Bm\"de dc|\"G\"B/2dB/2 Bc|\"C\"dc cB-|\"C\"eG2G|B2c|B4|\"Am\"c3/2d/2 ec|\\\n",
    "\n",
    "\"G\"G4-|\n",
    "\n",
    "\"G\"B3/2c/2 -d2|\"C\"=ee c3/2a/2|\"G\"B/2^A/2B/2g/2 gf/2d/2|\\\n",
    "\n",
    "\"D7\"d2 c2| \"G\"G2-|D/2D/2E/2d/2 =B3/2d/2|\n",
    "\n",
    "\"G\"dd d||\n",
    "\n",
    "K:G\n",
    "\n",
    "\"G7\"b2 \"A\"a2|\"G\"g4|\"G\"a/2g/2f/2e/2 de|\"D7/b\"d2 \"G\"AG|\"C\"E/2D/2E \"G\"Dd|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we have been successful in producing valid ABC notation by training a text generating LSTM(RNN) model with a ABC notation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
